---
date: 2017-4-22 15:10
status: public
title: '[笔记]0.machine-learning-第0篇笔记'
layout: post
tag: [机器学习, coursera_MachineLearning]
categories: [machine-learning]
description: 
---

* 目录
{:toc}

# 说明

这个是我在学习Stanford在coursera上开的机器学习课程中做的一些笔记，才刚开始接触这一方面，大部分是记录课程主要内容，还有些是靠自己的理解写的，不一定准确。课程地址：<https://www.coursera.org/learn/machine-learning/home>

这一系列的笔记初衷是在学习这个课程的时候做一些记录，加深自己的理解，并且方便后续有什么问题的时候能够快速查找，确实后面这一点效果不错，这门课程的内容对目前的我来说帮助很大。后续可能会修改笔记的内容方便没有学过这门课的同学参考，但是目前来说**很不建议通过看这个笔记达到学习的目的**，如果实在觉得看课程视频太耗费时间（毕竟课程面向所有人，会讲的有点慢），有几个感觉可行的建议：看课程的字幕，或者看coursera上官方整理的笔记，这个[链接](https://www.coursera.org/learn/machine-learning/resources/JXWWS)点进去，左边一排Week x Lecture Notes就是的，不过好像要在coursera上注册这个课程，还有一个比较推荐的方式是看[alex的笔记](http://www.holehouse.org/mlclass/index.html)，他的笔记可能是除了课程本身之外最推荐学习这个课程的方式了，记录的非常详细，作者本身也是Washington University的博士，从笔记可以看出学霸与我等学渣的区别Q_Q。

这一系列笔记的权限是所有人可见，走过路过的孩子如果发现有什么错误或者不清楚的可以通过<lujunmo1992@126.com>找到我，虽然我比较懒不一定会改就是了。

# 关于课程

总的来说这门课程非常推荐作为入门课程，之前需要的基础很少，仅仅需要一些简单的编程以及线性代数和微积分的一些知识，不过数学基础不太好也关系不大，各种公式都是可以直接跳过推导的，这个课感觉也不太重视理论推导。但是对于一个公式或者模型，有一定的感觉比较重要（至少对我来说是这样）。比如一个cost function，看到公式要大概的清楚每一项有什么直观上的意义，比如说前面项是对预测错误进行的惩罚，后面是对过拟合进行的惩罚，对每个参数求导为什么会长这样，向量形式与用循环的直观写法有什么联系等等。好吧，我承认好多复杂一些的公式我是看不出来什么，但是我还是觉得这个很重要。

对于之前已经在机器学习领域打拼很久，但是没有受过系统训练的人来说，有些部分也很推荐看一下，这门课会针对很多困难情况给出一些可行的想法。比如说在发现数据量不够的情况下，是想办法花很多时间收集数据还是考虑是不是模型本身出了问题；怎么判断一个模型hyperparameter的选择是不是合适的（这是我目前认为机器学习最玄学的部分）；在一个机器学习pipeline效果不好的情况下应该从哪一方面着手分析问题等。重点来说就是第六周和最后两周的课程，其他课程中好像会穿插提到一些，但是这三周感觉有点多。

这门课程设计了基本的机器学习算法（线性回归、逻辑回归、SVM、神经网络、K-Means等），然后会针对参数的选择和结果优化给出一些建议，后续有大规模数据量情况下的算法。对于神经网络这部分介绍比较少，也没有深入的介绍深度学习的各种架构。课程系统非常完善，视频中会有一些问题，小节结束后会有题目进行简单的测验，整个课程一共八个编程作业，感觉难度不大，会有详细的帮助文档，编程作业也有评分系统，使用的是matlab或者octave。

# 关于机器学习

作为一个还没怎么入门的初学者，我只能凭我目前的感觉说说对机器学习的理解。我感觉机器学习是通过大量的数据寻找样本与结果之间的联系，利用优化算法训练出合适的模型。当然，随着后面的发展，演化出了深度学习以及各种架构，对这些都有着一定的改变，这个不在这门课的讨论范围。这门课中的模型都还是需要大量数据来训练的。

## feature选择

对于很难通过直接编程解决的问题，可以尝试用大量的数据让机器在数据之中找到与结果的规律，从而对于没见过的数据也能达到很好的预测结果。这里有很多值得注意的地方，一个是数据中需要有规律，虽然这个规律很难被人为发现，但是通过大量数据，和这些数据对应的结果，机器能从中找到联系。对于没有规律的数据，机器怎么学习估计都学不会，比如给一系列的随机数，让机器猜下一个随机数是什么，这个难度就很大了，所以在机器学习中，对于feature的选择和处理我感觉是很重要的，由于现在机器还不够智能，我们要帮助他们去掉与结果没关系的feature，或者对现有的feature进行一些处理，让机器更容易在有限的数据中收敛。

> 我觉得feature的选择很重要，虽然理论上来说，只要数据够多，模型选择正确的情况下，随便怎么学习都是可以收敛的，但是一般来讲，数据量不会无限大，为了防止数据用完了模型还没有收敛，需要人工观察feature来进一步优化，多余的feature一定会对训练过程造成不必要的干扰。

## 模型选择和优化算法

其次，对于目前的机器学习来说，我们需要选择一种模型，告诉机器每个样本喂给这个模型之后能输出我们想要的结果，只是一开始不清楚这个模型的参数，所以机器需要从大量的数据中找出规律，让参数尽可能接近真实模型的那个参数。至于这个参数怎么才能接近真实的参数，就涉及到优化的算法了，比如梯度下降等，找到一个合适的优化算法也是机器学习领域的一个课题，不过只是想运用机器学习这个技术的话，可以直接使用已经实作好了的算法，一般来说还挺有效的。

## 评价学习结果

这门课中我学习到很重要的一点就是：人的感觉及其不靠谱，所以很多时候我们拿到一个结果都需要一个量化指标。那我自己举例，训练好的一个模型之后，为了看模型的结果，我可能拿100个测试用例跑这个模型，并且把真实结果和预测结果一个个的列出来，然后对比感觉这个结果怎么样，然后修改下参数训练模型再跑一次同样的测试用例，对比两次模型的结果。但是这样的方式是非常不靠谱的，因为我记不住每个模型跑测试用例的结果，并且我没有一个客观的数据来评价两次结果的好坏，可能最终影响我判断的只是其中的一两个数据，或者干脆蒙一个结果告诉别人这个模型好一些。但是这非常缺乏说服力。这门课程中介绍了一些判断模型好坏的指标，并且给出了每种方式下指标的优缺点，感觉这些在其他很多领域都是适用的。

## 遇到瓶颈时的思路

这个与前面一点类似，都是防止自己被直觉带上不归路。模型跑出来的结果不如预期的时候需要有一系列的方式分析问题出在哪里，对应不同的改进措施，不能凭借直觉在不停的调参数。虽然调下参数重新跑一遍模型很简单，运气好就能碰巧解决问题，但是这个是不科学的做法。想想以前调bug的时候，我是直接看到报错的地方，然后一步一步往前看，或者是从操作的第一步开始往后翻代码，不光是花时间，并且对于解决问题没有啥帮助，简单来说就是越陷越深了，以为问题出现在这一部分代码中，其实最后发现问题代码在很远的位置。直觉和经验有时候能快速帮我们定位问题，但是有时候还是要冷静下来仔细分析问题出在哪里。回到机器学习中来，就是结果不如预期的时候，怀疑是数据量问题，就看下cost function关于数据量的曲线，看结果是高方差(variance)还是高偏差(Bias)，多余的数据量有没有帮助，算法有没有收敛等。总的来说就是遇到问题不要单凭直觉纠结好久。

# 所有笔记链接
- [[笔记]0.machine-learning-第0篇笔记](http://junmo.farbox.com/post/ji-qi-xue-xi/-bi-ji-0.machine-learning-di-0pian-bi-ji)
- [[笔记]1.machine-learning-介绍](http://junmo.farbox.com/post/ji-qi-xue-xi/-bi-ji-1.machine-learning-jie-shao)
- [[笔记]2.machine-learning-多元线性回归](http://junmo.farbox.com/post/ji-qi-xue-xi/-bi-ji-2.machine-learning-duo-yuan-xian-xing-hui-gui)
- [[笔记]3.machine-learning-分类](http://junmo.farbox.com/post/ji-qi-xue-xi/-bi-ji-3.machine-learning-fen-lei)
- [[笔记]4.machine-learning-神经网络](http://junmo.farbox.com/post/ji-qi-xue-xi/-bi-ji-4.machine-learning-shen-jing-wang-luo)
- [[笔记]5.machine-learning-成本函数和反向传播算法](http://junmo.farbox.com/post/ji-qi-xue-xi/-bi-ji-5.machine-learning-cheng-ben-han-shu-he-fan-xiang-chuan-bo-suan-fa)
- [[笔记]6.machine-learning-应用机器学习的建议](http://junmo.farbox.com/post/ji-qi-xue-xi/-bi-ji-6.machine-learning-ying-yong-ji-qi-xue-xi-de-jian-yi)
- [[笔记]7.machine-learning-支持向量机](http://junmo.farbox.com/post/ji-qi-xue-xi/-bi-ji-7.machine-learning-zhi-chi-xiang-liang-ji)
- [[笔记]8.machine-learning-分类](http://junmo.farbox.com/post/ji-qi-xue-xi/-bi-ji-8.machine-learning-fen-lei)
- [[笔记]9.machine-learning-异常检测](http://junmo.farbox.com/post/ji-qi-xue-xi/-bi-ji-9.machine-learning-yi-chang-jian-ce)
- [[笔记]10.machine-learning-大数据量下的梯度下降算法](http://junmo.farbox.com/post/ji-qi-xue-xi/-bi-ji-10.machine-learning-da-shu-ju-liang-xia-de-ti-du-xia-jiang-suan-fa)
- [[笔记]11.machine-learning-应用示例:照片中的光学字符识别](http://junmo.farbox.com/post/ji-qi-xue-xi/-bi-ji-11.machine-learning-ying-yong-shi-li-zhao-pian-zhong-de-guang-xue-zi-fu-shi-bie)